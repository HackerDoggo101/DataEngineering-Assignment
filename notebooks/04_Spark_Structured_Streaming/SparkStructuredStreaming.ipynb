{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "786a122c-e5ab-4343-a14f-2109091c5e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Author: Yam Jason"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0f51a117-ec9e-4e8e-b953-e1628cf4a051",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(\"Spark\")\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8cad918f-0e08-4547-81ff-ede106917fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/07 15:30:36 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "sc = spark.sparkContext\n",
    "sc.addFile(\"../../de_classes/data_storage/hadoop_file_handler.py\")\n",
    "\n",
    "\n",
    "from hadoop_file_handler import HadoopFileHandler\n",
    "\n",
    "handler = HadoopFileHandler()\n",
    "df = handler.read_json('data/predictions/predictions3.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7905bd18-6a27-4fe0-9359-7bc0082bfd69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('Review', StringType(), True), StructField('prediction', DoubleType(), True)])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataSchema = df.schema\n",
    "dataSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d323549-54be-464e-8826-39bb801cac99",
   "metadata": {},
   "source": [
    "## Create the Data Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e0f19cbb-61f0-44eb-abbd-2bb8cc632a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "streaming = spark.readStream.schema(dataSchema).option(\"maxFilesPerTrigger\", 1)\\\n",
    "  .json(\"data/predictions/predictions3.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f610e177-4707-4458-b2d8-167912a376d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "activityCounts = streaming.groupBy(\n",
    "    \"Prediction\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2d902b64-881c-41ad-834f-d18c3d77fc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the shuffle partitions to a small value\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d80b1e68-4d8c-4d7a-a7e7-afda26d8916e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/07 15:30:39 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-b64d15ae-fe8e-4688-bb22-2e11a678a741. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "24/09/07 15:30:39 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    }
   ],
   "source": [
    "activityQuery = activityCounts.writeStream.queryName(\"activity_counts\")\\\n",
    "  .format(\"memory\").outputMode(\"complete\")\\\n",
    "  .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "73e1574d-dd21-4cd0-8f1f-a86aeb960471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|Prediction|count|\n",
      "+----------+-----+\n",
      "|       0.0|   16|\n",
      "|       2.0|   89|\n",
      "+----------+-----+\n",
      "\n",
      "+----------+-----+\n",
      "|Prediction|count|\n",
      "+----------+-----+\n",
      "|       0.0|   35|\n",
      "|       2.0|  235|\n",
      "+----------+-----+\n",
      "\n",
      "+----------+-----+\n",
      "|Prediction|count|\n",
      "+----------+-----+\n",
      "|       0.0|   53|\n",
      "|       2.0|  342|\n",
      "+----------+-----+\n",
      "\n",
      "+----------+-----+\n",
      "|Prediction|count|\n",
      "+----------+-----+\n",
      "|       0.0|   59|\n",
      "|       2.0|  374|\n",
      "+----------+-----+\n",
      "\n",
      "+----------+-----+\n",
      "|Prediction|count|\n",
      "+----------+-----+\n",
      "|       0.0|   59|\n",
      "|       2.0|  374|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from time import sleep\n",
    "for i in range(5):\n",
    "    spark.sql(\"SELECT * FROM activity_counts\").show()\n",
    "    sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5097aebd-b258-4a2e-8fec-4fda4ac2d23b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<pyspark.sql.streaming.query.StreamingQuery at 0x7f0a2016a800>]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.streams.active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bed28ca3-9a2f-405e-b12e-b65c4efda812",
   "metadata": {},
   "outputs": [],
   "source": [
    "activityQuery.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5a45a117-b090-49d9-bd91-3a0fee452cf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/07 15:30:46 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-2ee562be-acd5-48cd-99ca-a578b851da7d. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "24/09/07 15:30:46 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr, when\n",
    "\n",
    "# Filter where the Prediction is 2.0\n",
    "positiveFilter = streaming.withColumn(\"Sentiment\", when(expr(\"Prediction = 2.0\"), \"Positive\"))\\\n",
    "  .where(\"Sentiment = 'Positive'\")\\\n",
    "  .where(\"Prediction is not null\")\\\n",
    "  .select(\"Review\", \"Sentiment\")\\\n",
    "  .writeStream\\\n",
    "  .queryName(\"positive_filter\")\\\n",
    "  .format(\"memory\")\\\n",
    "  .outputMode(\"append\")\\\n",
    "  .start()\n",
    "\n",
    "# Wait for termination\n",
    "#positiveFilter.awaitTermination()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d8d49e9d-bedf-41f5-abdd-a07b2cda6160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+\n",
      "|Review|Sentiment|\n",
      "+------+---------+\n",
      "+------+---------+\n",
      "\n",
      "+--------------------+---------+\n",
      "|              Review|Sentiment|\n",
      "+--------------------+---------+\n",
      "|  has arrived thanks| Positive|\n",
      "|good quality deli...| Positive|\n",
      "|                best| Positive|\n",
      "|             perfect| Positive|\n",
      "|good packing very...| Positive|\n",
      "|good good good go...| Positive|\n",
      "|        good packing| Positive|\n",
      "|                good| Positive|\n",
      "|        ok good item| Positive|\n",
      "|                good| Positive|\n",
      "|lazada u should r...| Positive|\n",
      "|                good| Positive|\n",
      "|item received goo...| Positive|\n",
      "|second time i bou...| Positive|\n",
      "|fast shipment sel...| Positive|\n",
      "|fast delivery goo...| Positive|\n",
      "|        good product| Positive|\n",
      "|seller s delivery...| Positive|\n",
      "|good delivery goo...| Positive|\n",
      "|                good| Positive|\n",
      "+--------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------------------+---------+\n",
      "|              Review|Sentiment|\n",
      "+--------------------+---------+\n",
      "|  has arrived thanks| Positive|\n",
      "|good quality deli...| Positive|\n",
      "|                best| Positive|\n",
      "|             perfect| Positive|\n",
      "|good packing very...| Positive|\n",
      "|good good good go...| Positive|\n",
      "|        good packing| Positive|\n",
      "|                good| Positive|\n",
      "|        ok good item| Positive|\n",
      "|                good| Positive|\n",
      "|lazada u should r...| Positive|\n",
      "|                good| Positive|\n",
      "|item received goo...| Positive|\n",
      "|second time i bou...| Positive|\n",
      "|fast shipment sel...| Positive|\n",
      "|fast delivery goo...| Positive|\n",
      "|        good product| Positive|\n",
      "|seller s delivery...| Positive|\n",
      "|good delivery goo...| Positive|\n",
      "|                good| Positive|\n",
      "+--------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Note: this may take a while\n",
    "for i in range(3):\n",
    "    spark.sql(\"SELECT * FROM positive_filter\").show()\n",
    "    sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dd10566d-62d1-4f9c-8ddd-2a32c72ec148",
   "metadata": {},
   "outputs": [],
   "source": [
    "positiveFilter.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0bfaced0-3a25-47aa-93a1-541759712786",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/07 15:30:49 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-1d8fa11f-92a8-42dc-ae4e-2dc4057b05d7. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "24/09/07 15:30:49 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr, when\n",
    "\n",
    "# Filter where the Prediction is 2.0\n",
    "negativeFilter = streaming.withColumn(\"Sentiment\", when(expr(\"Prediction = 0.0\"), \"Negative\"))\\\n",
    "  .where(\"Sentiment = 'Negative'\")\\\n",
    "  .where(\"Prediction is not null\")\\\n",
    "  .select(\"Review\", \"Sentiment\")\\\n",
    "  .writeStream\\\n",
    "  .queryName(\"negative_filter\")\\\n",
    "  .format(\"memory\")\\\n",
    "  .outputMode(\"append\")\\\n",
    "  .start()\n",
    "\n",
    "# Wait for termination\n",
    "#positiveFilter.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7c91ae41-1a5a-468f-a15d-1b4c20743264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+\n",
      "|Review|Sentiment|\n",
      "+------+---------+\n",
      "+------+---------+\n",
      "\n",
      "+--------------------+---------+\n",
      "|              Review|Sentiment|\n",
      "+--------------------+---------+\n",
      "|packs black are r...| Negative|\n",
      "|tng ewallet auto ...| Negative|\n",
      "|quality no so goo...| Negative|\n",
      "|        poor quality| Negative|\n",
      "|dark red become pink| Negative|\n",
      "|receive good cond...| Negative|\n",
      "|                thin| Negative|\n",
      "|ear strings broke...| Negative|\n",
      "|bought brand mask...| Negative|\n",
      "|received quantity...| Negative|\n",
      "|very disappointed...| Negative|\n",
      "|good use thin com...| Negative|\n",
      "|wrong colour but ...| Negative|\n",
      "|             not bad| Negative|\n",
      "|doesnt match vide...| Negative|\n",
      "|i bought each col...| Negative|\n",
      "|ear strings broke...| Negative|\n",
      "|your mask not goo...| Negative|\n",
      "|second time i ord...| Negative|\n",
      "|   time repeat order| Negative|\n",
      "+--------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------------------+---------+\n",
      "|              Review|Sentiment|\n",
      "+--------------------+---------+\n",
      "|packs black are r...| Negative|\n",
      "|tng ewallet auto ...| Negative|\n",
      "|quality no so goo...| Negative|\n",
      "|        poor quality| Negative|\n",
      "|dark red become pink| Negative|\n",
      "|receive good cond...| Negative|\n",
      "|                thin| Negative|\n",
      "|ear strings broke...| Negative|\n",
      "|bought brand mask...| Negative|\n",
      "|received quantity...| Negative|\n",
      "|very disappointed...| Negative|\n",
      "|good use thin com...| Negative|\n",
      "|wrong colour but ...| Negative|\n",
      "|             not bad| Negative|\n",
      "|doesnt match vide...| Negative|\n",
      "|i bought each col...| Negative|\n",
      "|ear strings broke...| Negative|\n",
      "|your mask not goo...| Negative|\n",
      "|second time i ord...| Negative|\n",
      "|   time repeat order| Negative|\n",
      "+--------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Note: this may take a while\n",
    "for i in range(3):\n",
    "    spark.sql(\"SELECT * FROM negative_filter\").show()\n",
    "    sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "02428029-1459-4c0e-99a9-2ca18f70ea0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "negativeFilter.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1ac1147f-56ef-4a38-b9bc-8f09664ac012",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eaaa170-f943-426e-bfd2-aa6c1887bb3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "de-venv",
   "language": "python",
   "name": "de-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
