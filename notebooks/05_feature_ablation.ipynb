{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15c2bb96-47b8-4f99-8cb4-ab4aa264c7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Author: Wong Yee En (feature ablation), Goh Boon Xiang (Hbase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7288413-bd24-4898-8a57-d9f7ecc81b68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/07 20:48:42 WARN Utils: Your hostname, WeirdSmile. resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "24/09/07 20:48:42 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/09/07 20:48:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(\"FeatureAblation\")\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ade268c-f587-41bb-b8fd-ffa582e6a9c6",
   "metadata": {},
   "source": [
    "## Retrieve Train Test Data from HBase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb598b6f-8c93-4751-ac10-deff359f2119",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext\n",
    "sc.addFile(\"/home/student/G3_B/de_classes/data_storage/hbase_handler.py\")\n",
    "from hbase_handler import HBaseHandler\n",
    "hbase_handler = HBaseHandler(host='localhost', port=9090)# Create an instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38b8ebfd-acdf-4699-9636-59f5830e3cc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/07 20:48:54 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+-------------+--------------------+----------------+\n",
      "|              Review|Sentiment|SkuInfo_index|              tokens|number_of_tokens|\n",
      "+--------------------+---------+-------------+--------------------+----------------+\n",
      "|received thanks g...|        2|          1.0|[received, thanks...|               6|\n",
      "|get coughing ever...|        0|          1.0|[get, coughing, e...|               8|\n",
      "|            nice you|        2|          4.0|         [nice, you]|               2|\n",
      "|translator object...|        2|          6.0|[translator, obje...|               7|\n",
      "|       very thin ply|        0|          0.0|   [very, thin, ply]|               3|\n",
      "+--------------------+---------+-------------+--------------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1382"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df  = hbase_handler.retrieve_from_hbase('train_data')\n",
    "\n",
    "# Show the retrieved DataFrame\n",
    "train_df.show(5)\n",
    "train_df.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b80d028-6f98-4406-b622-54d2fc60d081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+-------------+--------------------+----------------+\n",
      "|              Review|Sentiment|SkuInfo_index|              tokens|number_of_tokens|\n",
      "+--------------------+---------+-------------+--------------------+----------------+\n",
      "|                good|        2|          4.0|              [good]|               1|\n",
      "|repeat purchase c...|        2|          3.0|[repeat, purchase...|               8|\n",
      "|           less pack|        2|          0.0|        [less, pack]|               2|\n",
      "|colour not same a...|        2|          6.0|[colour, not, sam...|               7|\n",
      "|                good|        2|          1.0|              [good]|               1|\n",
      "+--------------------+---------+-------------+--------------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "644"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df  = hbase_handler.retrieve_from_hbase('test_data')\n",
    "\n",
    "# Show the retrieved DataFrame\n",
    "test_df.show(5)\n",
    "test_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d6b634d-f099-4843-badc-e0200336b07f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|Sentiment|count|\n",
      "+---------+-----+\n",
      "|        2| 1216|\n",
      "|        0|  166|\n",
      "+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df.groupBy('Sentiment').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3c3550e-b105-4441-8467-8c28d8e1db76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|Sentiment|count|\n",
      "+---------+-----+\n",
      "|        2|  575|\n",
      "|        0|   69|\n",
      "+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_df.groupBy('Sentiment').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c20789a-7228-4c06-8f6e-6a63b19be8e5",
   "metadata": {},
   "source": [
    "## Close Hbase Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "880417e8-27f4-408c-9732-b8ec0bea310b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the connection\n",
    "hbase_handler.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd7c3d9-cf78-41b6-a69b-5b09e2f36ebd",
   "metadata": {},
   "source": [
    "## Save train and test data files to json file as required by asgm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e2b9a40d-ab2f-48d4-9f71-0a59f379cd01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/07 20:49:13 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "sc.addFile(\"../de_classes/data_storage/hadoop_file_handler.py\")\n",
    "\n",
    "# Import the HadoopFileHandler class\n",
    "from hadoop_file_handler import HadoopFileHandler\n",
    "\n",
    "# Create an instance of HadoopFileHandler\n",
    "handler = HadoopFileHandler()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a19000-064a-4894-a3c7-1018dcae7dd7",
   "metadata": {},
   "source": [
    "### Write train_df to json (hadoop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69b52269-fc34-43c1-a045-fceb8acde1d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "output_path = \"data/train_test/train_json\"\n",
    "handler.write_json(train_df, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d403ee60-a1b2-4b05-8cf2-b0f963fcfd04",
   "metadata": {},
   "source": [
    "### Write test_df to json (hadoop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c139001d-0b1d-4ef8-a37c-7d8460efb617",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "output_path = \"data/train_test/test_json\"\n",
    "handler.write_json(test_df, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1875eb-4474-49af-9126-1ade3d3957d7",
   "metadata": {},
   "source": [
    "### Read train_df from json(hadoop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f535ffb-e786-4673-8d91-0aa4526e45bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = \"data/train_test/train_json\"\n",
    "train_df = handler.read_json(input_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4a691c-be4e-467e-8bca-3f6b622a8000",
   "metadata": {},
   "source": [
    "### Read test_df from json(hadoop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "253f7d4e-5dba-46c9-831e-f96de55667ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = \"data/train_test/test_json\"\n",
    "test_df = handler.read_json(input_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a82a67b-c32c-4d6f-9536-760c5c7b46f4",
   "metadata": {},
   "source": [
    "### Vectorization (TD-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb3ccfb9-1210-45d7-b720-9422a836385a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.addFile(\"../de_classes/data_preparation/data_transformation.py\")\n",
    "\n",
    "# Import the HadoopFileHandler class\n",
    "from data_transformation import DataTransformations\n",
    "\n",
    "# Create an instance of HadoopFileHandler\n",
    "transform = DataTransformations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "390e2813-ad5c-49c0-a2d8-68598333b264",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "tf_idf_train = DataTransformations.calculate_tfidf(train_df, tokens_col=\"tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "21da5b17-b685-495d-a23b-e559a3b08fd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+-------------+--------------------+----------------+--------------------+--------------------+\n",
      "|              Review|Sentiment|SkuInfo_index|              tokens|number_of_tokens|         rawFeatures|            features|\n",
      "+--------------------+---------+-------------+--------------------+----------------+--------------------+--------------------+\n",
      "|received thanks g...|        2|          1.0|[received, thanks...|               6|(10000,[3601,3958...|(10000,[3601,3958...|\n",
      "|get coughing ever...|        0|          1.0|[get, coughing, e...|               8|(10000,[1756,3506...|(10000,[1756,3506...|\n",
      "|            nice you|        2|          4.0|         [nice, you]|               2|(10000,[3370,4338...|(10000,[3370,4338...|\n",
      "|translator object...|        2|          6.0|[translator, obje...|               7|(10000,[293,2804,...|(10000,[293,2804,...|\n",
      "|       very thin ply|        0|          0.0|   [very, thin, ply]|               3|(10000,[2167,3944...|(10000,[2167,3944...|\n",
      "|                  ok|        2|          0.0|                [ok]|               1|(10000,[2645],[1.0])|(10000,[2645],[2....|\n",
      "|fast delivery qua...|        2|         10.0|[fast, delivery, ...|               4|(10000,[2645,3506...|(10000,[2645,3506...|\n",
      "|fast delivery goo...|        2|         33.0|[fast, delivery, ...|               4|(10000,[3601,6168...|(10000,[3601,6168...|\n",
      "|  batch quality thin|        2|          1.0|[batch, quality, ...|               3|(10000,[2167,3506...|(10000,[2167,3506...|\n",
      "|well packed wrapp...|        2|         18.0|[well, packed, wr...|               7|(10000,[157,3370,...|(10000,[157,3370,...|\n",
      "|       delivery fast|        2|          8.0|    [delivery, fast]|               2|(10000,[7128,9263...|(10000,[7128,9263...|\n",
      "|waterproof breath...|        2|          1.0|[waterproof, brea...|               9|(10000,[760,1382,...|(10000,[760,1382,...|\n",
      "|                good|        2|         16.0|              [good]|               1|(10000,[6168],[1.0])|(10000,[6168],[0....|\n",
      "|fast delivery nic...|        2|         13.0|[fast, delivery, ...|              12|(10000,[1140,1916...|(10000,[1140,1916...|\n",
      "|received good con...|        2|          4.0|[received, good, ...|               3|(10000,[3601,5328...|(10000,[3601,5328...|\n",
      "|good products gre...|        2|          1.0|[good, product, g...|               8|(10000,[447,750,1...|(10000,[447,750,1...|\n",
      "|parcel has been r...|        2|          6.0|[parcel, ha, been...|              10|(10000,[938,1687,...|(10000,[938,1687,...|\n",
      "|i received goods ...|        2|          2.0|[i, received, goo...|               6|(10000,[1687,1756...|(10000,[1687,1756...|\n",
      "|all good fast del...|        2|          3.0|[all, good, fast,...|               9|(10000,[80,867,41...|(10000,[80,867,41...|\n",
      "|ok received good ...|        2|          4.0|[ok, received, go...|               4|(10000,[2645,3601...|(10000,[2645,3601...|\n",
      "+--------------------+---------+-------------+--------------------+----------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tf_idf_train.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0aa41d0a-22a6-4fc9-879b-f4510b0f2268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|              Review|            features|\n",
      "+--------------------+--------------------+\n",
      "|received thanks g...|(10000,[3601,3958...|\n",
      "|get coughing ever...|(10000,[1756,3506...|\n",
      "|            nice you|(10000,[3370,4338...|\n",
      "|translator object...|(10000,[293,2804,...|\n",
      "|       very thin ply|(10000,[2167,3944...|\n",
      "|                  ok|(10000,[2645],[2....|\n",
      "|fast delivery qua...|(10000,[2645,3506...|\n",
      "|fast delivery goo...|(10000,[3601,6168...|\n",
      "|  batch quality thin|(10000,[2167,3506...|\n",
      "|well packed wrapp...|(10000,[157,3370,...|\n",
      "|       delivery fast|(10000,[7128,9263...|\n",
      "|waterproof breath...|(10000,[760,1382,...|\n",
      "|                good|(10000,[6168],[0....|\n",
      "|fast delivery nic...|(10000,[1140,1916...|\n",
      "|received good con...|(10000,[3601,5328...|\n",
      "|good products gre...|(10000,[447,750,1...|\n",
      "|parcel has been r...|(10000,[938,1687,...|\n",
      "|i received goods ...|(10000,[1687,1756...|\n",
      "|all good fast del...|(10000,[80,867,41...|\n",
      "|ok received good ...|(10000,[2645,3601...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tf_idf_train.select(\"Review\", \"features\").show(truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3af91bb5-2d4a-403e-8854-b1ea2bde6803",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_test = DataTransformations.calculate_tfidf(test_df, tokens_col=\"tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9492a746-79f0-4a41-8943-274105ae42ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|              Review|            features|\n",
      "+--------------------+--------------------+\n",
      "|                good|(10000,[6168],[0....|\n",
      "|repeat purchase c...|(10000,[80,747,40...|\n",
      "|           less pack|(10000,[1195,6547...|\n",
      "|colour not same a...|(10000,[2525,4041...|\n",
      "|                good|(10000,[6168],[0....|\n",
      "|good delivery goo...|(10000,[80,447,39...|\n",
      "|good product fast...|(10000,[447,1738,...|\n",
      "|condition okay go...|(10000,[747,1226,...|\n",
      "|mask very thick a...|(10000,[3048,3944...|\n",
      "|second time buy p...|(10000,[80,3446,8...|\n",
      "|perfect size adul...|(10000,[72,520,13...|\n",
      "|received good con...|(10000,[2067,3506...|\n",
      "|goods have been s...|(10000,[1299,1485...|\n",
      "|           terbaekkk|(10000,[5163],[5....|\n",
      "|        ok good item|(10000,[1916,2645...|\n",
      "|          well worth|(10000,[157,9679]...|\n",
      "|good product fast...|(10000,[447,524,3...|\n",
      "|very good seller ...|(10000,[1709,2478...|\n",
      "|so bad i asked ma...|(10000,[141,387,1...|\n",
      "|price cheap but b...|(10000,[1738,1989...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tf_idf_test.select(\"Review\", \"features\").show(truncate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e52ec72-cb45-4821-817d-866bbd5a88da",
   "metadata": {},
   "source": [
    "# Handle Class imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c6edbe90-b425-4d5b-b80e-3af010074f05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|Sentiment|count|\n",
      "+---------+-----+\n",
      "|        2| 1216|\n",
      "|        0|  166|\n",
      "+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tf_idf_train.groupBy('Sentiment').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3ea9ec-4a9d-435a-82bf-1547366c1de8",
   "metadata": {},
   "source": [
    "## Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d5e3b61c-a851-49eb-9a81-468b7a93f84e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Sentiment Counts:\n",
      "Majority class count (Sentiment 2): 1216\n",
      "Minority class count (Sentiment 0): 166\n"
     ]
    }
   ],
   "source": [
    "print(\"Initial Sentiment Counts:\")\n",
    "\n",
    "# Separate the majority and minority classes\n",
    "major_df = tf_idf_train.filter(tf_idf_train.Sentiment == 2)\n",
    "minor_df = tf_idf_train.filter(tf_idf_train.Sentiment == 0)\n",
    "\n",
    "# Check the counts of each class\n",
    "major_count = major_df.count()\n",
    "minor_count = minor_df.count()\n",
    "\n",
    "print(f\"Majority class count (Sentiment 2): {major_count}\")\n",
    "print(f\"Minority class count (Sentiment 0): {minor_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "509fceaf-4068-4e86-bb7c-cd4492a83232",
   "metadata": {},
   "outputs": [],
   "source": [
    "oversampled_train = DataTransformations.oversample(tf_idf_train, label_col=\"Sentiment\", majority_label=2, minority_label=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ddea6270-0366-4fab-80f8-ae66cb16070a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Sentiment Counts after Oversampling:\n",
      "+---------+-----+\n",
      "|Sentiment|count|\n",
      "+---------+-----+\n",
      "|        2| 1216|\n",
      "|        0| 1162|\n",
      "+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Combined Sentiment Counts after Oversampling:\")\n",
    "oversampled_train.groupBy('Sentiment').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fa9eeba1-0953-495a-8ef3-9f7c4a6353c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Review: string (nullable = true)\n",
      " |-- Sentiment: integer (nullable = true)\n",
      " |-- SkuInfo_index: double (nullable = true)\n",
      " |-- tokens: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- number_of_tokens: integer (nullable = true)\n",
      " |-- rawFeatures: vector (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "oversampled_train.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c615f431-6b94-4f53-9067-f307926c4ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Review: string (nullable = true)\n",
      " |-- Sentiment: integer (nullable = true)\n",
      " |-- SkuInfo_index: double (nullable = true)\n",
      " |-- tokens: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- number_of_tokens: integer (nullable = true)\n",
      " |-- rawFeatures: vector (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tf_idf_test.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0249b513-bf74-4bd0-94c2-80e60f59777b",
   "metadata": {},
   "source": [
    "# Modeling + Feature Ablations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d3f1765b-e1aa-4774-b4d6-075ec72a6645",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.addFile(\"../de_classes/modelling.py\")\n",
    "\n",
    "# Import the HadoopFileHandler class\n",
    "from modelling import ModelTrainer\n",
    "\n",
    "# Create an instance of HadoopFileHandler\n",
    "modelTrainer = ModelTrainer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4e60fd-da2a-4dfa-a297-18f961d377be",
   "metadata": {},
   "source": [
    "## Model 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c6552a-09c8-4fb5-9be0-077c969033a2",
   "metadata": {},
   "source": [
    "### Feature: X1 = features(reviews after tf-idf) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ace3d229-9f97-4893-876e-b4a1f0b8df6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8633540372670807\n",
      "Precision: 0.8836300986058025\n",
      "Recall: 0.8633540372670807\n",
      "F1 Score: 0.8720196979522022\n"
     ]
    }
   ],
   "source": [
    "# Prepare features with only TF-IDF features\n",
    "train_df1= ModelTrainer.prepare_features(oversampled_train, feature_cols=[\"features\"])\n",
    "test_df1= ModelTrainer.prepare_features(tf_idf_test, feature_cols=[\"features\"])\n",
    "\n",
    "# Train the model and evaluate\n",
    "model1 = ModelTrainer.train_model(train_df1, label_col=\"Sentiment\")\n",
    "predictions1, accuracy_1, precision_1, recall_1, f1_score_1 = ModelTrainer.evaluate_model(model1, test_df1, label_col=\"Sentiment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a151463b-d1a8-4733-86e9-e5805b36ef90",
   "metadata": {},
   "source": [
    "## Model 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21230c95-e608-4a2e-b568-733e90203d7d",
   "metadata": {},
   "source": [
    "### Features: X1 = features, X2 = number_of_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3e18ed8e-a72c-402f-99d5-dfea33bd65b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8571428571428571\n",
      "Precision: 0.8802215689205575\n",
      "Recall: 0.8571428571428571\n",
      "F1 Score: 0.8671032153227644\n"
     ]
    }
   ],
   "source": [
    "# Feature ablation with TF-IDF features and StarCount\n",
    "train_df2 = ModelTrainer.prepare_features(oversampled_train, feature_cols=[\"features\",\"number_of_tokens\"])\n",
    "test_df2 = ModelTrainer.prepare_features(tf_idf_test, feature_cols=[\"features\",\"number_of_tokens\"])\n",
    "\n",
    "model2 = ModelTrainer.train_model(train_df2, label_col=\"Sentiment\")\n",
    "predictions2, accuracy_2, precision_2, recall_2, f1_score_2 = ModelTrainer.evaluate_model(model2, test_df2, label_col=\"Sentiment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae9c4e1-8488-498c-a530-30f0d9eb0350",
   "metadata": {},
   "source": [
    "## Model 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0885dc09-b175-40ff-af46-bcabd6f1a345",
   "metadata": {},
   "source": [
    "## Features: X1 = features, X2 = SkuInfo_index, X3 = number_of_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e229d5be-99d9-4e95-ab17-0e9828a39cdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8773291925465838\n",
      "Precision: 0.8889660076891068\n",
      "Recall: 0.8773291925465838\n",
      "F1 Score: 0.8826397957280104\n"
     ]
    }
   ],
   "source": [
    "# Prepare features\n",
    "feature_columns = [\"features\", \"SkuInfo_index\",\"number_of_tokens\"]\n",
    "train_df3 = ModelTrainer.prepare_features(oversampled_train, feature_cols=feature_columns)\n",
    "test_df3 = ModelTrainer.prepare_features(tf_idf_test, feature_cols=feature_columns)\n",
    "\n",
    "model3 = ModelTrainer.train_model(train_df3, label_col=\"Sentiment\")\n",
    "predictions3, accuracy_3, precision_3, recall_3, f1_score_3 = ModelTrainer.evaluate_model(model3, test_df3, label_col=\"Sentiment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2807fb3d-f628-48ab-866e-b004bb87a73c",
   "metadata": {},
   "source": [
    "## Comparison tables for all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0d82c4cb-13f7-4a4f-a44e-d3da04e8477f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create a list of tuples representing the metrics for each model\n",
    "data = [\n",
    "    (\"Model1\", accuracy_1, precision_1, recall_1, f1_score_1),\n",
    "    (\"Model2\", accuracy_2, precision_2, recall_2, f1_score_2),\n",
    "    (\"Model3\", accuracy_3, precision_3, recall_3, f1_score_3)\n",
    "]\n",
    "\n",
    "# Step 2: Create a DataFrame with columns for Model, Accuracy, Precision, Recall, F1Score\n",
    "df = spark.createDataFrame(data, [\"Model\", \"Accuracy\", \"Precision\", \"Recall\", \"F1Score\"])\n",
    "\n",
    "# Step 3: Register the DataFrame as a SQL temporary view\n",
    "df.createOrReplaceTempView(\"model_metrics\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9cdc368a-81a2-4f33-bbbc-2eb1bbb551aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------+------+-------+\n",
      "|Model |Accuracy|Precision|Recall|F1Score|\n",
      "+------+--------+---------+------+-------+\n",
      "|Model1|0.8634  |0.8836   |0.8634|0.872  |\n",
      "|Model2|0.8571  |0.8802   |0.8571|0.8671 |\n",
      "|Model3|0.8773  |0.889    |0.8773|0.8826 |\n",
      "+------+--------+---------+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Use Spark SQL to query and present the data, rounding to 2 decimal places\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        Model,\n",
    "        ROUND(Accuracy, 4) AS Accuracy,\n",
    "        ROUND(Precision, 4) AS Precision,\n",
    "        ROUND(Recall, 4) AS Recall,\n",
    "        ROUND(F1Score, 4) AS F1Score\n",
    "    FROM model_metrics\n",
    "\"\"\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "41c56af4-72cf-456a-b39f-6db4bca77e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predictions3.select(\"Review\", \"prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e0ae7c3e-a26f-40f9-971a-6c2eeab3ad54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------+----------+\n",
      "|Review                                                                      |prediction|\n",
      "+----------------------------------------------------------------------------+----------+\n",
      "|good                                                                        |2.0       |\n",
      "|repeat purchase comfortable value buy fast shipping delivery                |2.0       |\n",
      "|less pack                                                                   |2.0       |\n",
      "|colour not same advertised meltblown between waterproof                     |0.0       |\n",
      "|good                                                                        |2.0       |\n",
      "|good delivery good products next time buy again thanks                      |2.0       |\n",
      "|good product fast delivery satisfied good response reasonable price         |2.0       |\n",
      "|condition okay good many times repeat order kids like                       |2.0       |\n",
      "|mask very thick according face not how big                                  |0.0       |\n",
      "|second time buy praise                                                      |2.0       |\n",
      "|perfect size adults comfortable fit stylish design convenient earloop design|2.0       |\n",
      "|received good condition package quality looks good yet try                  |2.0       |\n",
      "|goods have been securely accepted number enough i am satisfied              |2.0       |\n",
      "|terbaekkk                                                                   |2.0       |\n",
      "|ok good item                                                                |2.0       |\n",
      "|well worth                                                                  |2.0       |\n",
      "|good product fast delivery days cheaper product much affordable             |2.0       |\n",
      "|very good seller responsive friendly highly recommended                     |2.0       |\n",
      "|so bad i asked mask didn t go his nose split mask me i wanted change        |0.0       |\n",
      "|price cheap but bit thin                                                    |2.0       |\n",
      "+----------------------------------------------------------------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cb765e9a-0eda-4907-a008-2acc2d038757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------------------------------------------------------------------+----------+\n",
      "|Review                                                                                                                                    |prediction|\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------+----------+\n",
      "|colour not same advertised meltblown between waterproof                                                                                   |0.0       |\n",
      "|mask very thick according face not how big                                                                                                |0.0       |\n",
      "|so bad i asked mask didn t go his nose split mask me i wanted change                                                                      |0.0       |\n",
      "|i ordered pcs there can be                                                                                                                |0.0       |\n",
      "|quality not really abit thin but colour same pic what i expect                                                                            |0.0       |\n",
      "|strong smell lar can t hold                                                                                                               |0.0       |\n",
      "|very nice thank you seller                                                                                                                |0.0       |\n",
      "|very thin claimed be korea but they are made china                                                                                        |0.0       |\n",
      "|less pack mask order pack up packs only                                                                                                   |0.0       |\n",
      "|top seller always well done                                                                                                               |0.0       |\n",
      "|i order pcs bt i get pcs only i wan refund                                                                                                |0.0       |\n",
      "|thin                                                                                                                                      |0.0       |\n",
      "|toooo thin maak                                                                                                                           |0.0       |\n",
      "|received but top not zipped properly order light green but colour difference what s sell also dark red also pink                          |0.0       |\n",
      "|your mask not good hurt people all                                                                                                        |0.0       |\n",
      "|seller did not send blue color mask hope buy again                                                                                        |0.0       |\n",
      "|normal quality delivery fast but wrong colour given bought navy but given other blue pls double check again so same mistake can be prevent|0.0       |\n",
      "|goods are up good condition t thank you pd sell flash my delivery fast                                                                    |0.0       |\n",
      "|shop happy ja son purchase deliver quickly                                                                                                |0.0       |\n",
      "|less one pack pink mask bought packs pink mask but only get packs masks have strong chemical smell poor quality                           |0.0       |\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter predictions to show only rows where prediction is 0\n",
    "predictions.filter(predictions.prediction == 0).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7ebaf0c8-7d11-43d2-a25c-15d9fd838cdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/07 20:52:52 WARN SparkContext: The path ../de_classes/data_storage/hadoop_file_handler.py has been added already. Overwriting of added paths is not supported in the current version.\n"
     ]
    }
   ],
   "source": [
    "sc.addFile(\"../de_classes/data_storage/hadoop_file_handler.py\")\n",
    "\n",
    "# Import the HadoopFileHandler class\n",
    "from hadoop_file_handler import HadoopFileHandler\n",
    "\n",
    "# Create an instance of HadoopFileHandler\n",
    "handler = HadoopFileHandler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "81541cee-dbfb-4607-b1a9-0cc8f53f62c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Save the entire DataFrame to a JSON file\n",
    "handler.write_json(predictions, \"data/predictions/predictions3.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c159fcb1-f892-4a8e-9079-06fe3c74f03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2237e0-f1bf-497b-ab67-3862fee5b10e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "de-venv",
   "language": "python",
   "name": "de-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
